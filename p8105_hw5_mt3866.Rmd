---
title: "p8105_hw5_mt3866"
output: github_document
date: "2025-11-13"
---

Load libraries
```{r}
library(tidyverse)
library(rvest)
library(broom)
library(purrr)
```

Q1

Write a function that randomly draws birthdays out of 365 days 
```{r}
bday_sim = function(n_room) {
  
  birthdays = sample(1:365, n_room, replace = TRUE)
  repeated_bday = length(unique(birthdays)) < n_room
  repeated_bday
}

```

Run the function 10,000 times for each group size 2 to 50
```{r}
bday_sim_results = 
  expand_grid(
    bdays = 2:50, 
    iter = 1:10000
  ) |> 
  mutate(
    result = map_lgl(bdays, bday_sim)
  ) |> 
  group_by(
    bdays
  ) |> 
  summarize(
    prob_repeat = mean(result)
  )
```

Plotting the results
```{r}
bday_sim_results |> 
  ggplot(aes(x = bdays, y = prob_repeat)) + 
  geom_point() + 
  geom_line() +
    labs(
    x = "group size",
    y = "probability"
  )
```
This plot shows that as the group size increases, the probability that at least two people in the group will share a birthday increases as well. These two variables seem to have a direct and positive relationship with each other.

Q2

Follow the mean, sd, and n requirement, generate 5,000 samples.
Repeat for mu = 1 to 6 and test null
```{r}
sim_test = function(n = 30, mu = 0, sigma = 5, alpha = 0.05) {
x = rnorm(n = n, mean = mu, sd = sigma)
  t_res = t.test (x, mu = 0)
  tidy(t_res) |> 
    select(estimate, p.value) |> 
    rename(mu_hat = estimate)
}


sim_results =
  expand_grid(
  mu = 0:6 ,
  iter = 1:5000) |>
  mutate(res = map2(mu, iter, ~sim_test (n = 30, mu = .x, sigma = 5)))|>
  unnest(res)

power_res = sim_results |>
  group_by(mu) |>
  summarize(power = mean(p.value < 0.05))
```

Create plot of power and mean
```{r}
ggplot(power_res, aes(x = mu, y = power)) +
  geom_point(size = 2) +
  geom_line() +
  labs(
    x = "true value of mean",
    y = "power",
    title = "true value of mean vs power"
  )
```
As we can see in the plot, power increases as the true mean of the sample increases. With a larger effect size, it's easier to detect differences which increases the power as the significance of our results will increase.

Plot average mean vs true value, overlay another plot
```{r}
mu_summary = sim_results |>
  group_by(mu) |>
  summarize(
    avg_mu_hat = mean(mu_hat),
    avg_rejected_mu_hat = mean(mu_hat[p.value < 0.05])
  )

mu_summary_long = mu_summary |>
  pivot_longer(
    cols = c(avg_mu_hat, avg_rejected_mu_hat),
    names_to = "type",
    values_to = "average_mu_hat"
  )

ggplot(mu_summary_long, aes(x = mu, y = average_mu_hat, color = type)) +
  geom_point(size = 2) +
  geom_line() +
  scale_color_manual(
    values = c("avg_mu_hat" = "blue", "avg_rejected_mu_hat" = "red"),
    labels = c("All samples", "Rejected H0")
  ) +
  labs(
    x = "true mu",
    y = "average estimated mu",
    color = "sample type",
    title = "average estimate of mu hat versus true mu"
  ) +
  theme_minimal()
```
The sample average mean for tests where the null is rejected is approximately the same generally, however is a bit higher for lower sample averages. The reason for this is because when the mu is small, only the more extreme values will get rejected and this brings selection bias, so the rejected null line is biased upwards in the beginning of the plot in comparison to the all samples line.

Q3

To describe the raw data, there are multiple variable such as the reported date of homicide, victim's first and last name (separately), their race, age, sex, city, state, latitude, longitude, and disposition. There are 52,179 total observations in the data set.

Import homicide data & make a city_state column
```{r}
  homicides = read_csv("homicide-data.csv")|>
  janitor::clean_names()|>
  mutate(city_state = str_c(city, state, sep = ","))
```

Summarize by city and obtain total number of homicides and unsolved homicides

```{r}
city_summary = homicides |>
  group_by(city_state) |>
  summarize(
    total_homicides = n(),
    unsolved_homicides = sum(disposition %in% c("Open/No arrest")
  )) |>
  arrange(desc(total_homicides))

city_summary
  
```

Estimate proportion of unsolved homicides. Save, use broom::tidy and pull proportion and CI

```{r}
baltimore = city_summary |>
  filter(city_state == "Baltimore,MD")
baltimore_prop = prop.test(
  x = baltimore$unsolved_homicides,
  n = baltimore$total_homicides
)
baltimore_tidy = broom::tidy(baltimore_prop)|>
  mutate(city_state = "Baltimore,MD")
baltimore_tidy |>
  select(city_state, estimate, conf.low, conf.high)|>
  knitr::kable(digits = 3)
```

Run for each city and extract
```{r}
city_props = city_summary|>
  mutate(prop_test = map2(unsolved_homicides, total_homicides, ~prop.test(.x, .y))
  )|>
  mutate(tidy_res = map(prop_test, broom::tidy))|>
  unnest(tidy_res)|>
  select(city_state, estimate, conf.low, conf.high) |>
  knitr::kable(digits = 3)
  
  city_props
```

Create the plot

