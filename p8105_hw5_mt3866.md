p8105_hw5_mt3866
================
2025-11-13

Load libraries

``` r
library(tidyverse)
```

    ## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──
    ## ✔ dplyr     1.1.4     ✔ readr     2.1.5
    ## ✔ forcats   1.0.0     ✔ stringr   1.5.1
    ## ✔ ggplot2   3.5.2     ✔ tibble    3.3.0
    ## ✔ lubridate 1.9.4     ✔ tidyr     1.3.1
    ## ✔ purrr     1.1.0     
    ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
    ## ✖ dplyr::filter() masks stats::filter()
    ## ✖ dplyr::lag()    masks stats::lag()
    ## ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors

``` r
library(rvest)
```

    ## 
    ## Attaching package: 'rvest'
    ## 
    ## The following object is masked from 'package:readr':
    ## 
    ##     guess_encoding

``` r
library(broom)
```

Q1

Write a function that randomly draws birthdays out of 365 days & run the
function 10000 times for each group size 2-50

``` r
sim_birthday = function (n) {
birthdays = sample(1:365, n, replace = TRUE)
any(duplicated(birthdays))
}
birthday_results = expand_grid(
  groupsize = 2:50,
  iter = 1:10000
)|>
  mutate(duplicate=map_lgl(groupsize, sim_birthday))|>
  group_by(groupsize)|>
  summarize(prob=mean(duplicate))
```

Make a plot

``` r
ggplot(birthday_results, aes(x=groupsize, y=prob)) +
  geom_point() +
  labs(
    x = "group size",
    y = "probability"
  )
```

![](p8105_hw5_mt3866_files/figure-gfm/unnamed-chunk-3-1.png)<!-- -->
This plot shows that as the group size increases, the probability that
at least two people in the group will share a birthday increases as
well. These two variables seem to have a direct and positive
relationship with each other.

Q2

Follow the mean, sd, and n requirement, generate 5,000 samples. Repeat
for mu = 1 to 6 and test null

``` r
sim_test = function(n = 30, mu = 0, sigma = 5, alpha = 0.05) {
x = rnorm(n = n, mean = mu, sd = sigma)
  t_res = t.test (x, mu = 0)
  tidy(t_res) |> 
    select(estimate, p.value) |> 
    rename(mu_hat = estimate)
}


sim_results =
  expand_grid(
  mu = 0:6 ,
  iter = 1:5000) |>
  mutate(res = map2(mu, iter, ~sim_test (n = 30, mu = .x, sigma = 5)))|>
  unnest(res)

power_res = sim_results |>
  group_by(mu) |>
  summarize(power = mean(p.value < 0.05))
```

Create plot of power and mean

``` r
ggplot(power_res, aes(x = mu, y = power)) +
  geom_point(size = 2) +
  geom_line() +
  labs(
    x = "true value of mean",
    y = "power",
    title = "true value of mean vs power"
  )
```

![](p8105_hw5_mt3866_files/figure-gfm/unnamed-chunk-5-1.png)<!-- --> As
we can see in the plot, power increases as the true mean of the sample
increases. With a larger effect size, it’s easier to detect differences
which increases the power as the significance of our results will
increase.

Plot average mean vs true value, overlay another plot

``` r
mu_summary = sim_results |>
  group_by(mu) |>
  summarize(
    avg_mu_hat = mean(mu_hat),
    avg_rejected_mu_hat = mean(mu_hat[p.value < 0.05])
  )

mu_summary_long = mu_summary |>
  pivot_longer(
    cols = c(avg_mu_hat, avg_rejected_mu_hat),
    names_to = "type",
    values_to = "average_mu_hat"
  )

ggplot(mu_summary_long, aes(x = mu, y = average_mu_hat, color = type)) +
  geom_point(size = 2) +
  geom_line() +
  scale_color_manual(
    values = c("avg_mu_hat" = "blue", "avg_rejected_mu_hat" = "red"),
    labels = c("All samples", "Rejected H0")
  ) +
  labs(
    x = "true mu",
    y = "average estimated mu",
    color = "sample type",
    title = "average estimate of mu hat versus true mu"
  ) +
  theme_minimal()
```

![](p8105_hw5_mt3866_files/figure-gfm/unnamed-chunk-6-1.png)<!-- --> The
sample average mean for tests where the null is rejected is
approximately the same generally, however is a bit higher for lower
sample averages. The reason for this is because when the mu is small,
only the more extreme values will get rejected and this brings selection
bias, so the rejected null line is biased upwards in the beginning of
the plot in comparison to the all samples line.

Q3

To describe the raw data, there are multiple variable such as the
reported date of homicide, victim’s first and last name (separately),
their race, age, sex, city, state, latitude, longitude, and disposition.
There are 52,179 total observations in the data set.

Import homicide data & make a city_state column

``` r
  homicides = read_csv("homicide-data.csv")|>
  janitor::clean_names()|>
  mutate(city_state = str_c(city, state, sep = ","))
```

    ## Rows: 52179 Columns: 12
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (9): uid, victim_last, victim_first, victim_race, victim_age, victim_sex...
    ## dbl (3): reported_date, lat, lon
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

Summarize by city and obtain total number of homicides and unsolved
homicides

``` r
city_summary = homicides |>
  group_by(city_state) |>
  summarize(
    total_homicides = n(),
    unsolved_homicides = sum(disposition %in% c("Open/No arrest")
  )) |>
  arrange(desc(total_homicides))
```
