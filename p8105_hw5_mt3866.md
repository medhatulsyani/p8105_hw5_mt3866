p8105_hw5_mt3866
================
2025-11-13

Load libraries

``` r
library(tidyverse)
```

    ## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──
    ## ✔ dplyr     1.1.4     ✔ readr     2.1.5
    ## ✔ forcats   1.0.0     ✔ stringr   1.5.1
    ## ✔ ggplot2   3.5.2     ✔ tibble    3.3.0
    ## ✔ lubridate 1.9.4     ✔ tidyr     1.3.1
    ## ✔ purrr     1.1.0     
    ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
    ## ✖ dplyr::filter() masks stats::filter()
    ## ✖ dplyr::lag()    masks stats::lag()
    ## ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors

``` r
library(rvest)
```

    ## 
    ## Attaching package: 'rvest'
    ## 
    ## The following object is masked from 'package:readr':
    ## 
    ##     guess_encoding

``` r
library(broom)
library(purrr)
```

Q1

Write a function that randomly draws birthdays out of 365 days

``` r
bday_sim = function(n_room) {
  
  birthdays = sample(1:365, n_room, replace = TRUE)
  repeated_bday = length(unique(birthdays)) < n_room
  repeated_bday
}
```

Run the function 10,000 times for each group size 2 to 50

``` r
bday_sim_results = 
  expand_grid(
    bdays = 2:50, 
    iter = 1:10000
  ) |> 
  mutate(
    result = map_lgl(bdays, bday_sim)
  ) |> 
  group_by(
    bdays
  ) |> 
  summarize(
    prob_repeat = mean(result)
  )
```

Plotting the results

``` r
bday_sim_results |> 
  ggplot(aes(x = bdays, y = prob_repeat)) + 
  geom_point() + 
  geom_line() +
    labs(
    x = "group size",
    y = "probability"
  )
```

![](p8105_hw5_mt3866_files/figure-gfm/unnamed-chunk-4-1.png)<!-- -->
This plot shows that as the group size increases, the probability that
at least two people in the group will share a birthday increases as
well. These two variables seem to have a direct and positive
relationship with each other.

Q2

Follow the mean, sd, and n requirement, generate 5,000 samples. Repeat
for mu = 1 to 6 and test null

``` r
sim_test = function(n = 30, mu = 0, sigma = 5, alpha = 0.05) {
x = rnorm(n = n, mean = mu, sd = sigma)
  t_res = t.test (x, mu = 0)
  tidy(t_res) |> 
    select(estimate, p.value) |> 
    rename(mu_hat = estimate)
}


sim_results =
  expand_grid(
  mu = 0:6 ,
  iter = 1:5000) |>
  mutate(res = map2(mu, iter, ~sim_test (n = 30, mu = .x, sigma = 5)))|>
  unnest(res)

power_res = sim_results |>
  group_by(mu) |>
  summarize(power = mean(p.value < 0.05))
```

Create plot of power and mean

``` r
ggplot(power_res, aes(x = mu, y = power)) +
  geom_point(size = 2) +
  geom_line() +
  labs(
    x = "true value of mean",
    y = "power",
    title = "true value of mean vs power"
  )
```

![](p8105_hw5_mt3866_files/figure-gfm/unnamed-chunk-6-1.png)<!-- --> As
we can see in the plot, power increases as the true mean of the sample
increases. With a larger effect size, it’s easier to detect differences
which increases the power as the significance of our results will
increase.

Plot average mean vs true value, overlay another plot

``` r
mu_summary = sim_results |>
  group_by(mu) |>
  summarize(
    avg_mu_hat = mean(mu_hat),
    avg_rejected_mu_hat = mean(mu_hat[p.value < 0.05])
  )

mu_summary_long = mu_summary |>
  pivot_longer(
    cols = c(avg_mu_hat, avg_rejected_mu_hat),
    names_to = "type",
    values_to = "average_mu_hat"
  )

ggplot(mu_summary_long, aes(x = mu, y = average_mu_hat, color = type)) +
  geom_point(size = 2) +
  geom_line() +
  scale_color_manual(
    values = c("avg_mu_hat" = "blue", "avg_rejected_mu_hat" = "red"),
    labels = c("All samples", "Rejected H0")
  ) +
  labs(
    x = "true mu",
    y = "average estimated mu",
    color = "sample type",
    title = "average estimate of mu hat versus true mu"
  ) +
  theme_minimal()
```

![](p8105_hw5_mt3866_files/figure-gfm/unnamed-chunk-7-1.png)<!-- --> The
sample average mean for tests where the null is rejected is
approximately the same generally, however is a bit higher for lower
sample averages. The reason for this is because when the mu is small,
only the more extreme values will get rejected and this brings selection
bias, so the rejected null line is biased upwards in the beginning of
the plot in comparison to the all samples line.

Q3

To describe the raw data, there are multiple variable such as the
reported date of homicide, victim’s first and last name (separately),
their race, age, sex, city, state, latitude, longitude, and disposition.
There are 52,179 total observations in the data set.

Import homicide data & make a city_state column

``` r
  homicides = read_csv("homicide-data.csv")|>
  janitor::clean_names()|>
  mutate(city_state = str_c(city, state, sep = ","))
```

    ## Rows: 52179 Columns: 12
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (9): uid, victim_last, victim_first, victim_race, victim_age, victim_sex...
    ## dbl (3): reported_date, lat, lon
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

Summarize by city and obtain total number of homicides and unsolved
homicides

``` r
city_summary = homicides |>
  group_by(city_state) |>
  summarize(
    total_homicides = n(),
    unsolved_homicides = sum(disposition %in% c("Open/No arrest", "Closed without arrest")
  )) |>
  arrange(desc(total_homicides))

city_summary
```

    ## # A tibble: 51 × 3
    ##    city_state      total_homicides unsolved_homicides
    ##    <chr>                     <int>              <int>
    ##  1 Chicago,IL                 5535               4073
    ##  2 Philadelphia,PA            3037               1360
    ##  3 Houston,TX                 2942               1493
    ##  4 Baltimore,MD               2827               1825
    ##  5 Detroit,MI                 2519               1482
    ##  6 Los Angeles,CA             2257               1106
    ##  7 St. Louis,MO               1677                905
    ##  8 Dallas,TX                  1567                754
    ##  9 Memphis,TN                 1514                483
    ## 10 New Orleans,LA             1434                930
    ## # ℹ 41 more rows

Estimate proportion of unsolved homicides. Save, use broom::tidy and
pull proportion and CI

``` r
baltimore = city_summary |>
  filter(city_state == "Baltimore,MD")
baltimore_prop = prop.test(
  x = baltimore$unsolved_homicides,
  n = baltimore$total_homicides
)
baltimore_tidy = broom::tidy(baltimore_prop)|>
  mutate(city_state = "Baltimore,MD")
baltimore_tidy |>
  select(city_state, estimate, conf.low, conf.high)|>
  knitr::kable(digits = 3)
```

| city_state   | estimate | conf.low | conf.high |
|:-------------|---------:|---------:|----------:|
| Baltimore,MD |    0.646 |    0.628 |     0.663 |

Run for each city and extract

``` r
city_props = city_summary|>
  mutate(prop_test = map2(unsolved_homicides, total_homicides, ~prop.test(.x, .y))
  )|>
  mutate(tidy_res = map(prop_test, broom::tidy))|>
  unnest(tidy_res)|>
  select(city_state, estimate, conf.low, conf.high) 
```

    ## Warning: There was 1 warning in `mutate()`.
    ## ℹ In argument: `prop_test = map2(unsolved_homicides, total_homicides,
    ##   ~prop.test(.x, .y))`.
    ## Caused by warning in `prop.test()`:
    ## ! Chi-squared approximation may be incorrect

``` r
  city_props
```

    ## # A tibble: 51 × 4
    ##    city_state      estimate conf.low conf.high
    ##    <chr>              <dbl>    <dbl>     <dbl>
    ##  1 Chicago,IL         0.736    0.724     0.747
    ##  2 Philadelphia,PA    0.448    0.430     0.466
    ##  3 Houston,TX         0.507    0.489     0.526
    ##  4 Baltimore,MD       0.646    0.628     0.663
    ##  5 Detroit,MI         0.588    0.569     0.608
    ##  6 Los Angeles,CA     0.490    0.469     0.511
    ##  7 St. Louis,MO       0.540    0.515     0.564
    ##  8 Dallas,TX          0.481    0.456     0.506
    ##  9 Memphis,TN         0.319    0.296     0.343
    ## 10 New Orleans,LA     0.649    0.623     0.673
    ## # ℹ 41 more rows

Create the plot & arrange by estimate

``` r
organized_city = city_props|>
  arrange(estimate)|>
  mutate(city_state = factor(city_state, levels = city_state))

ggplot(organized_city, aes(x=city_state, y=estimate)) +
geom_point(size = 2) +
  geom_errorbar(aes(ymin=conf.low, ymax=conf.high), width = 0.3) +
                 labs(
                  x = "city",
                  y = "proportion",
                  title = "proportion of unsolved homicides by city with confidence intervals"
                ) +
                  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

![](p8105_hw5_mt3866_files/figure-gfm/unnamed-chunk-12-1.png)<!-- -->
